{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial for `mlcroissant` ðŸ¥"
      ],
      "metadata": {
        "id": "AriH9CP6AKhs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Croissant ðŸ¥ is a high-level format for machine learning datasets that combines metadata, resource file descriptions, data structure, and default ML semantics into a single file.\n",
        "\n",
        "Croissant builds on schema.org, and its `sc:Dataset` vocabulary, a widely used format to represent datasets on the Web, and make them searchable.\n",
        "\n",
        "The [`mlcroissant`](https://github.com/mlcommons/croissant/python/mlcroissant) Python library empowers developers to interact with Croissant:\n",
        "\n",
        "- Programmatically write your JSON-LD Croissant files.\n",
        "- Verify your JSON-LD Croissant files.\n",
        "- Load data from Croissant datasets."
      ],
      "metadata": {
        "id": "Hh-0cehIAErA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qpWrlwV-x52"
      },
      "outputs": [],
      "source": [
        "# https://github.com/mlcommons/croissant/python/mlcroissant\n",
        "\n",
        "!git clone https://github.com/mlcommons/croissant.git",
        "\n",
        "!cd croissant/python/mlcroissant",
        "\n",
        "!pip install -e .[git]",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example\n",
        "\n",
        "Let's try on a very concrete dataset: OpenAI's [`gpt-3`](https://github.com/openai/gpt-3) dataset for LLMs!\n",
        "\n",
        "In the tutorial, we will generate programmatically the Croissant JSON-LD file describing the dataset. Then we will verify the file and yield data from the dataset."
      ],
      "metadata": {
        "id": "Xwrol5JR_GTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlcroissant as mlc\n",
        "\n",
        "# FileObjects and FileSets define the resources of the dataset.\n",
        "distribution = [\n",
        "    # gpt-3 is hosted on a GitHub repository:\n",
        "    mlc.FileObject(\n",
        "        name=\"github-repository\",\n",
        "        description=\"OpenAI repository on GitHub.\",\n",
        "        content_url=\"https://github.com/openai/gpt-3\",\n",
        "        encoding_format=\"git+https\",\n",
        "        sha256=\"main\",\n",
        "    ),\n",
        "    # Within that repository, a FileSet lists all JSONL files:\n",
        "    mlc.FileSet(\n",
        "        name=\"jsonl-files\",\n",
        "        description=\"JSONL files are hosted on the GitHub repository.\",\n",
        "        contained_in=[\"github-repository\"],\n",
        "        encoding_format=\"application/jsonlines\",\n",
        "        includes=\"data/*.jsonl\",\n",
        "    ),\n",
        "]\n",
        "record_sets = [\n",
        "    # RecordSets contains records in the dataset.\n",
        "    mlc.RecordSet(\n",
        "        name=\"jsonl\",\n",
        "        # Each record has one or many fields...\n",
        "        fields=[\n",
        "            # Fields can be extracted from the FileObjects/FileSets.\n",
        "            mlc.Field(\n",
        "                name=\"context\",\n",
        "                description=\"\",\n",
        "                data_types=mlc.DataType.TEXT,\n",
        "                source=mlc.Source(\n",
        "                    uid=\"jsonl-files\",\n",
        "                    node_type=\"distribution\",\n",
        "                    # Extract the field from the column of a FileObject/FileSet:\n",
        "                    extract=mlc.Extract(column=\"context\"),\n",
        "                ),\n",
        "            ),\n",
        "            mlc.Field(\n",
        "                name=\"completion\",\n",
        "                description=\"The expected completion of the promt.\",\n",
        "                data_types=mlc.DataType.TEXT,\n",
        "                source=mlc.Source(\n",
        "                    uid=\"jsonl-files\",\n",
        "                    node_type=\"distribution\",\n",
        "                    extract=mlc.Extract(column=\"completion\"),\n",
        "                ),\n",
        "            ),\n",
        "            mlc.Field(\n",
        "                name=\"task\",\n",
        "                description=(\n",
        "                    \"The machine learning task appearing as the name of the\"\n",
        "                    \" file.\"\n",
        "                ),\n",
        "                data_types=mlc.DataType.TEXT,\n",
        "                source=mlc.Source(\n",
        "                    uid=\"jsonl-files\",\n",
        "                    node_type=\"distribution\",\n",
        "                    extract=mlc.Extract(\n",
        "                        file_property=mlc._src.structure_graph.nodes.source.FileProperty.filename\n",
        "                    ),\n",
        "                    # Extract the field from a regex on the filename:\n",
        "                    transforms=[mlc.Transform(regex=\"^(.*)\\\\.jsonl$\")],\n",
        "                ),\n",
        "            ),\n",
        "        ],\n",
        "    )\n",
        "]\n",
        "\n",
        "# Metadata contains information about the dataset.\n",
        "metadata = mlc.Metadata(\n",
        "    name=\"gpt-3\",\n",
        "    # Descriptions can contain plain text or markdown.\n",
        "    description=(\n",
        "        \"Recent work has demonstrated substantial gains on many NLP tasks and\"\n",
        "        \" benchmarks by pre-training on a large corpus of text followed by\"\n",
        "        \" fine-tuning on a specific task. While typically task-agnostic in\"\n",
        "        \" architecture, this method still requires task-specific fine-tuning\"\n",
        "        \" datasets of thousands or tens of thousands of examples. By contrast,\"\n",
        "        \" humans can generally perform a new language task from only a few\"\n",
        "        \" examples or from simple instructions \\u2013 something which current\"\n",
        "        \" NLP systems still largely struggle to do. Here we show that scaling\"\n",
        "        \" up language models greatly improves task-agnostic, few-shot\"\n",
        "        \" performance, sometimes even reaching competitiveness with prior\"\n",
        "        \" state-of-the-art fine-tuning approaches. Specifically, we train\"\n",
        "        \" GPT-3, an autoregressive language model with 175 billion parameters,\"\n",
        "        \" 10x more than any previous non-sparse language model, and test its\"\n",
        "        \" performance in the few-shot setting. For all tasks, GPT-3 is applied\"\n",
        "        \" without any gradient updates or fine-tuning, with tasks and few-shot\"\n",
        "        \" demonstrations specified purely via text interaction with the model.\"\n",
        "        \" GPT-3 achieves strong performance on many NLP datasets, including\"\n",
        "        \" translation, question-answering, and cloze tasks, as well as several\"\n",
        "        \" tasks that require on-the-fly reasoning or domain adaptation, such as\"\n",
        "        \" unscrambling words, using a novel word in a sentence, or performing\"\n",
        "        \" 3-digit arithmetic. At the same time, we also identify some datasets\"\n",
        "        \" where GPT-3's few-shot learning still struggles, as well as some\"\n",
        "        \" datasets where GPT-3 faces methodological issues related to training\"\n",
        "        \" on large web corpora. Finally, we find that GPT-3 can generate\"\n",
        "        \" samples of news articles which human evaluators have difficulty\"\n",
        "        \" distinguishing from articles written by humans. We discuss broader\"\n",
        "        \" societal impacts of this finding and of GPT-3 in general.\"\n",
        "    ),\n",
        "    citation=(\n",
        "        \"@article{brown2020language, title={Language Models are Few-Shot\"\n",
        "        \" Learners}, author={Tom B. Brown and Benjamin Mann and Nick Ryder and\"\n",
        "        \" Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind\"\n",
        "        \" Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and\"\n",
        "        \" Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom\"\n",
        "        \" Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and\"\n",
        "        \" Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and\"\n",
        "        \" Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and\"\n",
        "        \" Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford\"\n",
        "        \" and Ilya Sutskever and Dario Amodei}, year={2020},\"\n",
        "        \" eprint={2005.14165}, archivePrefix={arXiv}, primaryClass={cs.CL} }\"\n",
        "    ),\n",
        "    url=\"https://github.com/openai/gpt-3\",\n",
        "    distribution=distribution,\n",
        "    record_sets=record_sets,\n",
        ")\n"
      ],
      "metadata": {
        "id": "7OyQffJv-zso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When creating `Metadata`:\n",
        "- We also check for errors in the configuration.\n",
        "- We generate warnings if the configuration doesn't follow guidelines and best practices.\n",
        "\n",
        "For instance, in this case:"
      ],
      "metadata": {
        "id": "2RUVgWI-DldZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(metadata.issues.report())"
      ],
      "metadata": {
        "id": "AENcJUwMCd1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Property \"https://schema.org/license\" is recommended`...\n",
        "\n",
        "We can see at a glance that we miss an important metadata to build datasets for responsible AI: the license!"
      ],
      "metadata": {
        "id": "vES3KHaND4P2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the Croissant file and yield data\n",
        "\n",
        "Let's write the Croissant JSON-LD to a file on disk!"
      ],
      "metadata": {
        "id": "S0BEhzqiEjd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"croissant.json\", \"w\") as f:\n",
        "  content = metadata.to_json()\n",
        "  content = json.dumps(content, indent=2)\n",
        "  print(content)\n",
        "  f.write(content)\n",
        "  f.write(\"\\n\")  # Terminate file with newline"
      ],
      "metadata": {
        "id": "-XCycu81ECVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this JSON-LD file, we can easily create a dataset..."
      ],
      "metadata": {
        "id": "Ypb_ll3SE6UU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = mlc.Dataset(file=\"croissant.json\")"
      ],
      "metadata": {
        "id": "_JNyQFuAEiIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...and yield records from this dataset:"
      ],
      "metadata": {
        "id": "ldwdIGPoFT_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "records = dataset.records(record_set=\"jsonl\")\n",
        "\n",
        "for i, record in enumerate(records):\n",
        "  print(record)\n",
        "  if i > 10:\n",
        "    break"
      ],
      "metadata": {
        "id": "MHdVY4TBEqZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8a2sCy0GFYCQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
