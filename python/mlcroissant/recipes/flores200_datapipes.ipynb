{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a4763b9",
   "metadata": {},
   "source": [
    "# Using Croissant ü•ê, Hugging Face ü§ó, and DataPipes for FLORES200"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3a84fb",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df08e22",
   "metadata": {},
   "source": [
    "In this notebook, we'll show how Croissant can be used within the [PyTorch](https://pytorch.org/) framework to finetune a model using a [DataPipes](https://pytorch.org/data/main/torchdata.datapipes.iter.html) loader.\n",
    "We'll be using the [FLORES-200](https://github.com/facebookresearch/flores/blob/main/flores200/README.md) dataset with a [BERT](https://arxiv.org/abs/1810.04805) model hosted on [Hugging Face transformers](https://huggingface.co/bert-base-multilingual-cased).\n",
    "This notebook fine-tunes the BERT model to classify between the 200+ languages present in FLORES-200, also known as *language identification*.\n",
    "For the loader, we utilize a minimal DataPipe adapter on top of `mlcroissant` to help facilitate dataloading.\n",
    "\n",
    "First, let's start by installing some dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd14b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: git+https://github.com/mlcommons/croissant.git@main#subdirectory=python/mlcroissant&egg=mlcroissant[dev] contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: git+https://github.com/google/etils.git#egg=etils[epath] contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: git+https://github.com/google/etils.git#egg=etils[epath] contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install some needed dependencies for the notebook\n",
    "!apt-get install -y python3-dev graphviz libgraphviz-dev pkg-config\n",
    "!pip install --quiet \"git+https://github.com/${GITHUB_REPOSITORY:-mlcommons/croissant}.git@${GITHUB_HEAD_REF:-main}#subdirectory=python/mlcroissant&egg=mlcroissant[dev]\"\n",
    "!pip install --quiet torch torchvision torchaudio torchdata transformers numpy graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02331e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import time\n",
    "\n",
    "import mlcroissant as mlc\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchdata\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7052bd",
   "metadata": {},
   "source": [
    "Next, we need to provide the PyTorch adapter a Croissant file to read!\n",
    "We pass the FLORES-200 Croissant file to the class, which we will subsequently use to generate a PyTorch DataPipe for loading data.\n",
    "For convenience, we specify that certain fields should be automatically converted to their corresponding data type.\n",
    "We do this because Croissant implementations can return encoded data (e.g., UTF-8 bytes) for certain fields, but applications may expect decoded data (e.g., text strings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cbfa2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta_factory = mlc.torch.LoaderFactory(jsonld=\"../../../datasets/1.0/flores-200/metadata.json\")\n",
    "specification = {\n",
    "    \"translation\": mlc.torch.LoaderSpecificationDataType.INFER,\n",
    "    \"language\": mlc.torch.LoaderSpecificationDataType.INFER,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03576775",
   "metadata": {},
   "source": [
    "Using the adapter factory, we create a DataPipe.\n",
    "Importantly, we create one DataPipe per record set.\n",
    "In this case, this translates to a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d6be15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_pipe = ta_factory.as_datapipe(\n",
    "    record_set=\"language_translations_train_data_with_metadata\",\n",
    "    specification=specification,\n",
    ")\n",
    "test_data_pipe = ta_factory.as_datapipe(\n",
    "    record_set=\"language_translations_test_data_with_metadata\",\n",
    "    specification=specification,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115c94a9",
   "metadata": {},
   "source": [
    "Finally, we get a DataPipe, which we can use to manipulate data with traditional PyTorch tools!\n",
    "We can observe that these DataPipes are shallow wrappers around the base data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97fd5d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"115pt\" height=\"82pt\"\n",
       " viewBox=\"0.00 0.00 115.00 82.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 78)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-78 111,-78 111,4 -4,4\"/>\n",
       "<!-- Mapper&#45;682357334 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>Mapper&#45;682357334</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"80.5,-19 26.5,-19 26.5,0 80.5,0 80.5,-19\"/>\n",
       "<text text-anchor=\"middle\" x=\"53.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\">Mapper</text>\n",
       "</g>\n",
       "<!-- IterableWrapper&#45;682357343 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>IterableWrapper&#45;682357343</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"107,-74 0,-74 0,-55 107,-55 107,-74\"/>\n",
       "<text text-anchor=\"middle\" x=\"53.5\" y=\"-62\" font-family=\"monospace\" font-size=\"10.00\">IterableWrapper</text>\n",
       "</g>\n",
       "<!-- IterableWrapper&#45;682357343&#45;&gt;Mapper&#45;682357334 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>IterableWrapper&#45;682357343&#45;&gt;Mapper&#45;682357334</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.5,-54.75C53.5,-47.8 53.5,-37.85 53.5,-29.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"57,-29.09 53.5,-19.09 50,-29.09 57,-29.09\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x28ae6b670>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchdata.datapipes.utils.to_graph(train_data_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1186f9ef",
   "metadata": {},
   "source": [
    "## Setting Up The Data and Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac698f",
   "metadata": {},
   "source": [
    "The first thing we have to do to use the data in a classification setting is map each language, which represents the target class, to an integer.\n",
    "We'll quickly iterate the unique language descriptors to build this mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae257dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_to_int = {\n",
    "    y: i for i, y in enumerate(sorted(set(x[\"language\"] for x in train_data_pipe)))\n",
    "}\n",
    "int_to_classes = {i: y for (y, i) in classes_to_int.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aeafc15",
   "metadata": {},
   "source": [
    "Next, we need to set some hyperparameters that the rest of the code will use.\n",
    "The pretrained model, the sequence dimensions, and performance-related settings are specified here.\n",
    "Note we use an environment variable to override some defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f31a2b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "MAX_LENGTH = int(os.environ.get(\"MAX_SEQUENCE_LENGTH\", 10))\n",
    "LEARNING_RATE = 1e-5\n",
    "TRAIN_BATCH_SIZE = int(os.environ.get(\"TRAIN_BATCH_SIZE\", 128))\n",
    "TEST_BATCH_SIZE = int(os.environ.get(\"TEST_BATCH_SIZE\", 128))\n",
    "NUM_TRAIN_WORKERS = 0\n",
    "NUM_TEST_WORKERS = 0\n",
    "NUM_TRAIN_SAMPLES = 203388\n",
    "NUM_TEST_SAMPLES = 206448\n",
    "# 2 epochs is recommended for convergence\n",
    "NUM_EPOCHS = float(os.environ.get(\"NUM_TRAIN_EPOCHS\", 2.0))\n",
    "# Testing can similarly be subsampled\n",
    "FRACTION_TEST = float(os.environ.get(\"FRACTION_TEST\", 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7242166",
   "metadata": {},
   "source": [
    "Next, we have to specify how the tokenization should be done.\n",
    "We'll use the tokenizer that was used with the BERT model.\n",
    "We'll also apply some padding and truncation to the tensors to simplify the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cf44da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache(maxsize=1)\n",
    "def get_tokenizer():\n",
    "    tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "    return tokenizer\n",
    "\n",
    "def unpack_row(x):\n",
    "    tokenizer = get_tokenizer()\n",
    "    tokenized = tokenizer(\n",
    "        x[\"translation\"],\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    targets = classes_to_int[x[\"language\"]]\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"].reshape((-1,)),\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"].reshape((-1,)),\n",
    "        \"token_type_ids\": tokenized[\"token_type_ids\"].reshape((-1,)),\n",
    "        \"targets\": targets,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67db4a9e",
   "metadata": {},
   "source": [
    "Finally, we can create the train and test dataset.\n",
    "Note that the test dataset does not need shuffling.\n",
    "Otherwise, these two datasets are simply performing sharding and tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9444ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_dataset():\n",
    "    train_dataset = train_data_pipe\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=10000)\n",
    "    train_dataset = train_dataset.sharding_filter()\n",
    "    train_dataset = train_dataset.map(unpack_row)\n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "def get_test_dataset():\n",
    "    test_dataset = test_data_pipe\n",
    "    test_dataset = test_dataset.sharding_filter()\n",
    "    test_dataset = test_dataset.map(unpack_row)\n",
    "    return test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d13bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_train_dataset()\n",
    "test_dataset = get_test_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b51488b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"115pt\" height=\"247pt\"\n",
       " viewBox=\"0.00 0.00 115.00 247.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 243)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-243 111,-243 111,4 -4,4\"/>\n",
       "<!-- ShardingFilter&#45;288427495 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>ShardingFilter&#45;288427495</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"104,-74 3,-74 3,-55 104,-55 104,-74\"/>\n",
       "<text text-anchor=\"middle\" x=\"53.5\" y=\"-62\" font-family=\"monospace\" font-size=\"10.00\">ShardingFilter</text>\n",
       "</g>\n",
       "<!-- Mapper&#45;288437023 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>Mapper&#45;288437023</title>\n",
       "<polygon fill=\"#caff70\" stroke=\"black\" points=\"80.5,-19 26.5,-19 26.5,0 80.5,0 80.5,-19\"/>\n",
       "<text text-anchor=\"middle\" x=\"53.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\">Mapper</text>\n",
       "</g>\n",
       "<!-- ShardingFilter&#45;288427495&#45;&gt;Mapper&#45;288437023 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>ShardingFilter&#45;288427495&#45;&gt;Mapper&#45;288437023</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.5,-54.75C53.5,-47.8 53.5,-37.85 53.5,-29.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"57,-29.09 53.5,-19.09 50,-29.09 57,-29.09\"/>\n",
       "</g>\n",
       "<!-- Shuffler&#45;288437074 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>Shuffler&#45;288437074</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"86,-129 21,-129 21,-110 86,-110 86,-129\"/>\n",
       "<text text-anchor=\"middle\" x=\"53.5\" y=\"-117\" font-family=\"monospace\" font-size=\"10.00\">Shuffler</text>\n",
       "</g>\n",
       "<!-- Shuffler&#45;288437074&#45;&gt;ShardingFilter&#45;288427495 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>Shuffler&#45;288437074&#45;&gt;ShardingFilter&#45;288427495</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.5,-109.75C53.5,-102.8 53.5,-92.85 53.5,-84.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"57,-84.09 53.5,-74.09 50,-84.09 57,-84.09\"/>\n",
       "</g>\n",
       "<!-- Mapper&#45;682357334 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>Mapper&#45;682357334</title>\n",
       "<polygon fill=\"lightgrey\" stroke=\"black\" points=\"80.5,-184 26.5,-184 26.5,-165 80.5,-165 80.5,-184\"/>\n",
       "<text text-anchor=\"middle\" x=\"53.5\" y=\"-172\" font-family=\"monospace\" font-size=\"10.00\">Mapper</text>\n",
       "</g>\n",
       "<!-- Mapper&#45;682357334&#45;&gt;Shuffler&#45;288437074 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>Mapper&#45;682357334&#45;&gt;Shuffler&#45;288437074</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.5,-164.75C53.5,-157.8 53.5,-147.85 53.5,-139.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"57,-139.09 53.5,-129.09 50,-139.09 57,-139.09\"/>\n",
       "</g>\n",
       "<!-- IterableWrapper&#45;682357343 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>IterableWrapper&#45;682357343</title>\n",
       "<polygon fill=\"lightblue\" stroke=\"black\" points=\"107,-239 0,-239 0,-220 107,-220 107,-239\"/>\n",
       "<text text-anchor=\"middle\" x=\"53.5\" y=\"-227\" font-family=\"monospace\" font-size=\"10.00\">IterableWrapper</text>\n",
       "</g>\n",
       "<!-- IterableWrapper&#45;682357343&#45;&gt;Mapper&#45;682357334 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>IterableWrapper&#45;682357343&#45;&gt;Mapper&#45;682357334</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M53.5,-219.75C53.5,-212.8 53.5,-202.85 53.5,-194.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"57,-194.09 53.5,-184.09 50,-194.09 57,-194.09\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x113130f40>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchdata.datapipes.utils.to_graph(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3eee79",
   "metadata": {},
   "source": [
    "Now, we can build our model.\n",
    "We'll simply take the pretrained model and add a linear layer after pooling.\n",
    "Note that BERT has a feature dimension of 768. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f2fac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.base = BertModel.from_pretrained(MODEL_NAME)\n",
    "        self.fc = torch.nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        _, x = self.base(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=False,\n",
    "        )\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afbcd6d",
   "metadata": {},
   "source": [
    "We're almost there!\n",
    "Let's defined how to train and test the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f95fb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataloader, max_steps=None):\n",
    "    if max_steps is None:\n",
    "        max_steps = float(\"inf\")\n",
    "    elif max_steps <= 0:\n",
    "        return\n",
    "\n",
    "    model.train()\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    start_time = time.perf_counter()\n",
    "    for iteration, data in enumerate(dataloader, 1):\n",
    "        if iteration > max_steps:\n",
    "            break\n",
    "        input_ids = data[\"input_ids\"].to(device, dtype=torch.long, non_blocking=True)\n",
    "        attention_mask = data[\"attention_mask\"].to(\n",
    "            device, dtype=torch.long, non_blocking=True\n",
    "        )\n",
    "        token_type_ids = data[\"token_type_ids\"].to(\n",
    "            device, dtype=torch.long, non_blocking=True\n",
    "        )\n",
    "        targets = data[\"targets\"].to(device, dtype=torch.long, non_blocking=True)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        total_loss += len(outputs) * loss.item()\n",
    "        total_samples += len(outputs)\n",
    "        if iteration % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                mean_loss = total_loss / total_samples\n",
    "                elapsed_time = time.perf_counter() - start_time\n",
    "                samples_per_second = total_samples / elapsed_time\n",
    "                progress = total_samples / NUM_TRAIN_SAMPLES\n",
    "                msg = (\n",
    "                    f\"[Train {progress:.1%}]: loss: {mean_loss:.2f} \"\n",
    "                    f\"({samples_per_second:.2f} samples/sec)\"\n",
    "                )\n",
    "                print(msg)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    mean_loss = total_loss / total_samples\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "def test(model, dataloader, max_steps=None):\n",
    "    if max_steps is None:\n",
    "        max_steps = float(\"inf\")\n",
    "    elif max_steps <= 0:\n",
    "        return\n",
    "\n",
    "    model.eval()\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        start_time = time.perf_counter()\n",
    "        for iteration, data in enumerate(dataloader, 1):\n",
    "            if iteration > max_steps:\n",
    "                break\n",
    "            input_ids = data[\"input_ids\"].to(\n",
    "                device, dtype=torch.long, non_blocking=True\n",
    "            )\n",
    "            attention_mask = data[\"attention_mask\"].to(\n",
    "                device, dtype=torch.long, non_blocking=True\n",
    "            )\n",
    "            token_type_ids = data[\"token_type_ids\"].to(\n",
    "                device, dtype=torch.long, non_blocking=True\n",
    "            )\n",
    "            targets = data[\"targets\"].to(device, dtype=torch.long, non_blocking=True)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += len(outputs) * loss.item()\n",
    "            total_samples += len(outputs)\n",
    "            preds = torch.argmax(outputs, axis=1)\n",
    "            total_correct += torch.sum((preds == targets).float()).item()\n",
    "            if iteration % 100 == 0:\n",
    "                mean_loss = total_loss / total_samples\n",
    "                accuracy = total_correct / total_samples\n",
    "                elapsed_time = time.perf_counter() - start_time\n",
    "                samples_per_second = total_samples / elapsed_time\n",
    "                progress = total_samples / NUM_TEST_SAMPLES\n",
    "                msg = (\n",
    "                    f\"[Test {progress:.1%}]: loss: {mean_loss:.2f}, \"\n",
    "                    f\"accuracy: {accuracy:.1%} ({samples_per_second:.2f} samples/sec)\"\n",
    "                )\n",
    "                print(msg)\n",
    "        mean_loss = total_loss / total_samples\n",
    "        accuracy = total_correct / total_samples\n",
    "        return mean_loss, accuracy\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39ade59",
   "metadata": {},
   "source": [
    "Now we are ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd949981",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "model = BERTClassifier(len(classes_to_int))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47d08ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train 6.3%]: loss: 5.19 (223.96 samples/sec)\n",
      "[Train 12.6%]: loss: 4.87 (237.11 samples/sec)\n",
      "[Train 18.9%]: loss: 4.57 (243.10 samples/sec)\n",
      "[Train 25.2%]: loss: 4.31 (246.82 samples/sec)\n",
      "[Train 31.5%]: loss: 4.09 (249.75 samples/sec)\n",
      "[Train 37.8%]: loss: 3.89 (251.21 samples/sec)\n",
      "[Train 44.1%]: loss: 3.71 (252.13 samples/sec)\n",
      "[Train 50.3%]: loss: 3.54 (251.95 samples/sec)\n",
      "[Train 56.6%]: loss: 3.40 (252.14 samples/sec)\n",
      "[Train 62.9%]: loss: 3.26 (252.15 samples/sec)\n",
      "[Train 69.2%]: loss: 3.14 (252.27 samples/sec)\n",
      "[Train 75.5%]: loss: 3.04 (252.25 samples/sec)\n",
      "[Train 81.8%]: loss: 2.94 (252.22 samples/sec)\n",
      "[Train 88.1%]: loss: 2.84 (252.15 samples/sec)\n",
      "[Train 94.4%]: loss: 2.76 (252.08 samples/sec)\n",
      "[Test 6.2%]: loss: 1.49, accuracy: 62.6% (593.88 samples/sec)\n",
      "[Test 12.4%]: loss: 1.49, accuracy: 62.5% (702.67 samples/sec)\n",
      "[Test 18.6%]: loss: 1.46, accuracy: 62.9% (747.07 samples/sec)\n",
      "[Test 24.8%]: loss: 1.48, accuracy: 62.5% (772.61 samples/sec)\n",
      "[Test 31.0%]: loss: 1.47, accuracy: 62.5% (791.05 samples/sec)\n",
      "[Test 37.2%]: loss: 1.46, accuracy: 62.8% (801.36 samples/sec)\n",
      "[Test 43.4%]: loss: 1.45, accuracy: 63.3% (808.08 samples/sec)\n",
      "[Test 49.6%]: loss: 1.44, accuracy: 63.5% (812.20 samples/sec)\n",
      "[Test 55.8%]: loss: 1.43, accuracy: 63.7% (815.50 samples/sec)\n",
      "[Test 62.0%]: loss: 1.42, accuracy: 63.9% (820.52 samples/sec)\n",
      "[Test 68.2%]: loss: 1.41, accuracy: 64.1% (824.01 samples/sec)\n",
      "[Test 74.4%]: loss: 1.41, accuracy: 64.2% (827.59 samples/sec)\n",
      "[Test 80.6%]: loss: 1.40, accuracy: 64.3% (830.61 samples/sec)\n",
      "[Test 86.8%]: loss: 1.40, accuracy: 64.4% (832.17 samples/sec)\n",
      "[Test 93.0%]: loss: 1.39, accuracy: 64.5% (834.13 samples/sec)\n",
      "[Test 99.2%]: loss: 1.39, accuracy: 64.6% (836.10 samples/sec)\n",
      "Epoch 0 train_loss: 2.69, test_loss: 1.39, test_accuracy: 64.6%\n",
      "[Train 6.3%]: loss: 1.57 (223.58 samples/sec)\n",
      "[Train 12.6%]: loss: 1.51 (237.11 samples/sec)\n",
      "[Train 18.9%]: loss: 1.46 (242.06 samples/sec)\n",
      "[Train 25.2%]: loss: 1.41 (244.61 samples/sec)\n",
      "[Train 31.5%]: loss: 1.37 (247.15 samples/sec)\n",
      "[Train 37.8%]: loss: 1.34 (248.06 samples/sec)\n",
      "[Train 44.1%]: loss: 1.31 (248.76 samples/sec)\n",
      "[Train 50.3%]: loss: 1.28 (249.12 samples/sec)\n",
      "[Train 56.6%]: loss: 1.25 (249.49 samples/sec)\n",
      "[Train 62.9%]: loss: 1.23 (249.74 samples/sec)\n",
      "[Train 69.2%]: loss: 1.21 (249.89 samples/sec)\n",
      "[Train 75.5%]: loss: 1.19 (250.16 samples/sec)\n",
      "[Train 81.8%]: loss: 1.18 (250.23 samples/sec)\n",
      "[Train 88.1%]: loss: 1.16 (250.37 samples/sec)\n",
      "[Train 94.4%]: loss: 1.14 (250.36 samples/sec)\n",
      "[Test 6.2%]: loss: 0.96, accuracy: 71.9% (594.28 samples/sec)\n",
      "[Test 12.4%]: loss: 0.97, accuracy: 71.7% (702.78 samples/sec)\n",
      "[Test 18.6%]: loss: 0.95, accuracy: 72.0% (749.17 samples/sec)\n",
      "[Test 24.8%]: loss: 0.96, accuracy: 71.5% (774.53 samples/sec)\n",
      "[Test 31.0%]: loss: 0.96, accuracy: 71.5% (792.85 samples/sec)\n",
      "[Test 37.2%]: loss: 0.95, accuracy: 71.7% (803.12 samples/sec)\n",
      "[Test 43.4%]: loss: 0.93, accuracy: 72.2% (810.36 samples/sec)\n",
      "[Test 49.6%]: loss: 0.92, accuracy: 72.6% (813.56 samples/sec)\n",
      "[Test 55.8%]: loss: 0.91, accuracy: 72.8% (816.00 samples/sec)\n",
      "[Test 62.0%]: loss: 0.90, accuracy: 73.0% (820.74 samples/sec)\n",
      "[Test 68.2%]: loss: 0.89, accuracy: 73.2% (824.15 samples/sec)\n",
      "[Test 74.4%]: loss: 0.88, accuracy: 73.3% (827.57 samples/sec)\n",
      "[Test 80.6%]: loss: 0.88, accuracy: 73.4% (830.00 samples/sec)\n",
      "[Test 86.8%]: loss: 0.88, accuracy: 73.5% (831.44 samples/sec)\n",
      "[Test 93.0%]: loss: 0.87, accuracy: 73.6% (833.76 samples/sec)\n",
      "[Test 99.2%]: loss: 0.87, accuracy: 73.7% (835.66 samples/sec)\n",
      "Epoch 1 train_loss: 1.12, test_loss: 0.86, test_accuracy: 73.8%\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    num_workers=NUM_TRAIN_WORKERS,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    num_workers=NUM_TEST_WORKERS,\n",
    ")\n",
    "\n",
    "# We allow fractional epochs to speed up testing\n",
    "# So 1.5 epochs would be 1 epoch plus half the next epoch\n",
    "whole_epochs = int(NUM_EPOCHS)\n",
    "remainder_train_steps = (NUM_EPOCHS - whole_epochs) * NUM_TRAIN_SAMPLES / TRAIN_BATCH_SIZE\n",
    "max_test_steps = None\n",
    "if FRACTION_TEST != 1.0:\n",
    "    max_test_steps = FRACTION_TEST * NUM_TEST_SAMPLES / TEST_BATCH_SIZE\n",
    "\n",
    "epoch = 0  # Define epoch in case the following skips\n",
    "for epoch in range(whole_epochs):\n",
    "    train_loss = train(model, optimizer, train_dataloader)\n",
    "    test_loss, test_accuracy = test(model, test_dataloader, max_steps=max_test_steps)\n",
    "    print(\n",
    "        f\"Epoch {epoch} train_loss: {train_loss:.2f}, \"\n",
    "        f\"test_loss: {test_loss:.2f}, test_accuracy: {test_accuracy:.1%}\"\n",
    "    )\n",
    "if remainder_train_steps:\n",
    "    train_loss = train(model, optimizer, train_dataloader, max_steps=remainder_train_steps)\n",
    "    test_loss, test_accuracy = test(model, test_dataloader, max_steps=max_test_steps)\n",
    "    print(\n",
    "        f\"Epoch {epoch} train_loss: {train_loss:.2f}, \"\n",
    "        f\"test_loss: {test_loss:.2f}, test_accuracy: {test_accuracy:.1%}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b9b11a",
   "metadata": {},
   "source": [
    "## Trying It Out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b052160",
   "metadata": {},
   "source": [
    "Now that we have a model, we can see how it performs on some (perhaps) realistic examples üòÅ.\n",
    "Below, we pick a few translations of \"Croissants are tasty!\" to see how the model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da8c4a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cases = [\n",
    "    \"Croissants are tasty!\",\n",
    "    \"¬°Los croissants son deliciosos!\",\n",
    "    \"Les croissants sont savoureux!\",\n",
    "    \"ÁæäËßíÈù¢ÂåÖÂæàÂ•ΩÂêÉ!\",\n",
    "    \"◊î◊ß◊®◊ï◊ê◊°◊ï◊†◊ô◊ù ◊ò◊¢◊ô◊û◊ô◊ù!\",\n",
    "    \"Kruvasanlar juda mazali!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2859ac96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Croissants are tasty!' ‚Üí 'eng_Latn'\n",
      "'¬°Los croissants son deliciosos!' ‚Üí 'spa_Latn'\n",
      "'Les croissants sont savoureux!' ‚Üí 'fra_Latn'\n",
      "'ÁæäËßíÈù¢ÂåÖÂæàÂ•ΩÂêÉ!' ‚Üí 'zho_Hant'\n",
      "'◊î◊ß◊®◊ï◊ê◊°◊ï◊†◊ô◊ù ◊ò◊¢◊ô◊û◊ô◊ù!' ‚Üí 'heb_Hebr'\n",
      "'Kruvasanlar juda mazali!' ‚Üí 'uzn_Latn'\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer()\n",
    "for case in sample_cases:\n",
    "    toks = tokenizer(case, return_tensors=\"pt\")\n",
    "    logits = model(\n",
    "        toks[\"input_ids\"].to(device),\n",
    "        toks[\"attention_mask\"].to(device),\n",
    "        toks[\"token_type_ids\"].to(device),\n",
    "    )\n",
    "    y_pred = torch.argmax(logits, axis=1).item()\n",
    "    pred_lang = int_to_classes[y_pred]\n",
    "    print(f\"'{case}' ‚Üí '{pred_lang}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
