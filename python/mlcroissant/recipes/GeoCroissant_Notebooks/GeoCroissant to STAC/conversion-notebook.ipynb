{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd23ba6a-9757-49f3-ad54-8aefe4ebeeab",
   "metadata": {},
   "source": [
    "#  GeoCroissant to STAC Conversion\n",
    "\n",
    "<img src=\"../assets/GeoCroissant.jpg\" alt=\"GeoCroissant\" width=\"150\" style=\"float: right; margin-left: 50px;\">\n",
    "\n",
    "\n",
    "This notebook demonstrates how to convert **GeoCroissant metadata** — a geospatial extension of the Croissant metadata format — into a **STAC (SpatioTemporal Asset Catalog) Item**.\n",
    "\n",
    "### GeoCroissant includes:\n",
    "- Spatial information (geometry, bounding boxes)\n",
    "- Temporal coverage\n",
    "- Dataset structure (distributions, record sets)\n",
    "\n",
    "### By converting GeoCroissant to STAC:\n",
    "- Datasets become interoperable with geospatial tools and catalogs.\n",
    "- Metadata is structured using the **STAC specification**, enabling better discovery and analysis of spatial datasets.\n",
    "\n",
    "###  We use Python and `pystac` to:\n",
    "- Parse the GeoCroissant JSON.\n",
    "- Create a valid STAC Item with spatial and temporal context.\n",
    "- Add assets and structured data (e.g., imagery, annotations).\n",
    "- Save and validate the result with `stac-validator`.\n",
    "\n",
    "### Differences: STAC vs GeoCroissant\n",
    "\n",
    "| **STAC Field**             | **GeoCroissant Field**      | **Notes**                                                               |\n",
    "|----------------------------|------------------------------|-------------------------------------------------------------------------|\n",
    "| `id`                       | `@id`                        | Unique identifier for the dataset/item                                 |\n",
    "| `type`                     | `@type`                      | Usually `\"Feature\"` in STAC                                            |\n",
    "| `title`                    | `name`                       | Title of the dataset                                                   |\n",
    "| `description`              | `description`                | Dataset description                                                    |\n",
    "| `datetime`                 | `dct:temporal`               | Temporal coverage (inferred or computed)                               |\n",
    "| `bbox`                     | `geocr:BoundingBox`          | Spatial extent (bounding box)                                          |\n",
    "| `geometry`                 | `geocr:Geometry`             | Full spatial geometry (GeoJSON)                                        |\n",
    "| `assets`                   | `distribution`               | Resources related to the dataset                                       |\n",
    "| `assets[<key>].href`       | `contentUrl`                 | Link to the data asset                                                 |\n",
    "| `assets[<key>].type`       | `encodingFormat`             | Media type of the asset (e.g., `image/png`, `application/parquet`)     |\n",
    "| `properties[\"datetime\"]`   | *N/A*                        | Typically midpoint of the date range                                   |\n",
    "| `properties[\"spatial\"]`    | *N/A*                        | Not standardized in GeoCroissant; often inferred manually              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27708168-859f-412a-9376-a4171eae70d0",
   "metadata": {},
   "source": [
    "#  Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed40588d-dbb7-4cdf-bb63-ded3c4cabc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pystac\n",
      "  Downloading pystac-1.13.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pystac) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.7.0->pystac) (1.17.0)\n",
      "Downloading pystac-1.13.0-py3-none-any.whl (206 kB)\n",
      "Installing collected packages: pystac\n",
      "Successfully installed pystac-1.13.0\n",
      "Collecting stac-validator\n",
      "  Downloading stac_validator-3.10.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: requests>=2.32.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stac-validator) (2.32.5)\n",
      "Requirement already satisfied: jsonschema>=4.23.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stac-validator) (4.25.1)\n",
      "Requirement already satisfied: click>=8.1.8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stac-validator) (8.2.1)\n",
      "Requirement already satisfied: referencing>=0.35.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stac-validator) (0.36.2)\n",
      "Requirement already satisfied: pyYAML>=6.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from stac-validator) (6.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=4.23.0->stac-validator) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=4.23.0->stac-validator) (2025.4.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=4.23.0->stac-validator) (0.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from referencing>=0.35.1->stac-validator) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.3->stac-validator) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.3->stac-validator) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.3->stac-validator) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests>=2.32.3->stac-validator) (2025.8.3)\n",
      "Downloading stac_validator-3.10.1-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: stac-validator\n",
      "Successfully installed stac-validator-3.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pystac\n",
    "!pip install stac-validator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995e3749-8e64-4329-afd1-0629b5ae6663",
   "metadata": {},
   "source": [
    "# GeoCroissant to STAC Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcaf0f3a-6718-4dcb-8e1e-5d2275490c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAC item saved to stac_item.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Union\n",
    "from pystac import Item, Asset, MediaType\n",
    "from pystac.extensions.table import TableExtension\n",
    "from pystac.extensions.scientific import ScientificExtension\n",
    "\n",
    "# License mapping from URL\n",
    "KNOWN_LICENSES = {\n",
    "    \"https://creativecommons.org/licenses/by/4.0/\": \"CC-BY-4.0\",\n",
    "    \"https://choosealicense.com/licenses/cc-by-4.0/\": \"CC-BY-4.0\",\n",
    "    \"https://opensource.org/licenses/mit\": \"MIT\",\n",
    "    \"https://www.apache.org/licenses/license-2.0\": \"Apache-2.0\",\n",
    "    \"cc-by-4.0\": \"CC-BY-4.0\",\n",
    "    \"cc-by\": \"CC-BY-4.0\",\n",
    "}\n",
    "\n",
    "# CONUS bounding box (approximate)\n",
    "CONUS_BBOX = [-125.0, 24.0, -66.0, 50.0]\n",
    "CONUS_GEOMETRY = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [\n",
    "        [\n",
    "            [-125.0, 24.0],  # SW\n",
    "            [-125.0, 50.0],  # NW\n",
    "            [-66.0, 50.0],  # NE\n",
    "            [-66.0, 24.0],  # SE\n",
    "            [-125.0, 24.0],  # SW (close polygon)\n",
    "        ]\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "def extract_band_configuration(metadata: Dict) -> List[Dict]:\n",
    "    \"\"\"Extract band configuration from croissant metadata.\"\"\"\n",
    "    # Look for bandConfiguration in dataCollection\n",
    "    data_collection = metadata.get(\"dataCollection\", {})\n",
    "    band_config = data_collection.get(\"bandConfiguration\", {})\n",
    "\n",
    "    if not band_config:\n",
    "        # Look in other locations\n",
    "        for key, value in metadata.items():\n",
    "            if isinstance(value, dict) and \"bandConfiguration\" in value:\n",
    "                band_config = value[\"bandConfiguration\"]\n",
    "                break\n",
    "\n",
    "    bands = []\n",
    "    if band_config:\n",
    "        for band_key, band_info in band_config.items():\n",
    "            if isinstance(band_info, dict) and \"name\" in band_info:\n",
    "                bands.append(\n",
    "                    {\n",
    "                        \"name\": band_info[\"name\"],\n",
    "                        \"common_name\": band_info[\"name\"].lower(),\n",
    "                        \"hls_band\": band_info.get(\"hlsBand\", \"\"),\n",
    "                        \"wavelength\": band_info.get(\"wavelength\", \"\"),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return bands\n",
    "\n",
    "\n",
    "def extract_temporal_coverage(metadata: Dict) -> tuple:\n",
    "    \"\"\"Extract temporal coverage from croissant metadata.\"\"\"\n",
    "    # Check dataCollection first\n",
    "    data_collection = metadata.get(\"dataCollection\", {})\n",
    "    temporal_coverage = data_collection.get(\"temporalCoverage\", \"\")\n",
    "\n",
    "    if temporal_coverage:\n",
    "        # Parse \"2018-2021\" format\n",
    "        if \"-\" in temporal_coverage and len(temporal_coverage.split(\"-\")) == 2:\n",
    "            start_year, end_year = temporal_coverage.split(\"-\")\n",
    "            try:\n",
    "                start_datetime = datetime(int(start_year), 1, 1)\n",
    "                end_datetime = datetime(int(end_year), 12, 31)\n",
    "                midpoint_datetime = datetime(\n",
    "                    int(start_year) + (int(end_year) - int(start_year)) // 2, 6, 30\n",
    "                )\n",
    "                return start_datetime, end_datetime, midpoint_datetime\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    # Fallback to default values\n",
    "    return datetime(2018, 1, 1), datetime(2021, 12, 31), datetime(2019, 6, 30)\n",
    "\n",
    "\n",
    "def extract_spatial_coverage(metadata: Dict) -> tuple:\n",
    "    \"\"\"Extract spatial coverage from croissant metadata.\"\"\"\n",
    "    data_collection = metadata.get(\"dataCollection\", {})\n",
    "    spatial_coverage = data_collection.get(\"spatialCoverage\", \"\")\n",
    "\n",
    "    # For now, assume CONUS for this dataset\n",
    "    if \"United States\" in spatial_coverage or \"CONUS\" in spatial_coverage:\n",
    "        return CONUS_BBOX, CONUS_GEOMETRY\n",
    "\n",
    "    # Could be extended to parse other regions\n",
    "    return CONUS_BBOX, CONUS_GEOMETRY\n",
    "\n",
    "\n",
    "def normalize_license(license_raw: str) -> str:\n",
    "    \"\"\"Normalize license string to standard format.\"\"\"\n",
    "    if not license_raw:\n",
    "        return \"proprietary\"\n",
    "\n",
    "    license_key = license_raw.strip().lower()\n",
    "\n",
    "    # Direct mapping\n",
    "    if license_key in KNOWN_LICENSES:\n",
    "        return KNOWN_LICENSES[license_key]\n",
    "\n",
    "    # Pattern matching\n",
    "    if \"cc-by\" in license_key:\n",
    "        return \"CC-BY-4.0\"\n",
    "    elif \"mit\" in license_key:\n",
    "        return \"MIT\"\n",
    "    elif \"apache\" in license_key:\n",
    "        return \"Apache-2.0\"\n",
    "\n",
    "    return license_key.upper() if license_key else \"proprietary\"\n",
    "\n",
    "\n",
    "def extract_providers(metadata: Dict) -> List[Dict]:\n",
    "    \"\"\"Extract provider information from creator metadata.\"\"\"\n",
    "    providers = []\n",
    "\n",
    "    creator = metadata.get(\"creator\", {})\n",
    "    if isinstance(creator, list):\n",
    "        creators = creator\n",
    "    else:\n",
    "        creators = [creator] if creator else []\n",
    "\n",
    "    for creator_info in creators:\n",
    "        if isinstance(creator_info, dict):\n",
    "            provider = {\n",
    "                \"name\": creator_info.get(\"name\", \"Unknown\"),\n",
    "                \"roles\": [\"producer\"],\n",
    "            }\n",
    "            if creator_info.get(\"url\"):\n",
    "                provider[\"url\"] = creator_info[\"url\"]\n",
    "            providers.append(provider)\n",
    "\n",
    "    return providers\n",
    "\n",
    "\n",
    "def determine_media_type(href: str, asset_id: str, encoding_format: str = None) -> str:\n",
    "    \"\"\"Determine media type based on URL and format information.\"\"\"\n",
    "    href_lower = href.lower()\n",
    "    asset_id_lower = asset_id.lower()\n",
    "\n",
    "    if \"parquet\" in asset_id_lower or \"parquet\" in href_lower:\n",
    "        return MediaType.PARQUET\n",
    "    elif \"git\" in href_lower:\n",
    "        return \"application/git\"\n",
    "    elif \"tiff\" in href_lower or \"tif\" in href_lower:\n",
    "        return \"image/tiff\"\n",
    "    elif \"json\" in href_lower:\n",
    "        return MediaType.JSON\n",
    "    elif \"csv\" in href_lower:\n",
    "        return \"text/csv\"\n",
    "    elif \"huggingface\" in href_lower:\n",
    "        return MediaType.HTML\n",
    "    elif encoding_format:\n",
    "        return encoding_format\n",
    "\n",
    "    return MediaType.JSON\n",
    "\n",
    "\n",
    "def croissant_to_stac_item(croissant_json, output_path=None):\n",
    "    \"\"\"Convert Croissant metadata to STAC Item.\"\"\"\n",
    "    if isinstance(croissant_json, str):\n",
    "        metadata = json.loads(croissant_json)\n",
    "    else:\n",
    "        metadata = croissant_json\n",
    "\n",
    "    # Extract basic metadata\n",
    "    item_id = metadata.get(\"identifier\", metadata.get(\"name\", \"unknown-id\")).replace(\n",
    "        \"/\", \"_\"\n",
    "    )\n",
    "    title = metadata.get(\"name\", \"\")\n",
    "    description = metadata.get(\"description\", \"\")\n",
    "    license_raw = metadata.get(\"license\", \"proprietary\")\n",
    "    keywords = metadata.get(\"keywords\", [])\n",
    "    dataset_url = metadata.get(\"url\", \"\")\n",
    "    alternate_names = metadata.get(\"alternateName\", [])\n",
    "\n",
    "    # Normalize license\n",
    "    license_normalized = normalize_license(license_raw)\n",
    "\n",
    "    # Extract provider information\n",
    "    providers = extract_providers(metadata)\n",
    "\n",
    "    # Extract temporal and spatial coverage\n",
    "    start_datetime, end_datetime, midpoint_datetime = extract_temporal_coverage(\n",
    "        metadata\n",
    "    )\n",
    "    bbox, geometry = extract_spatial_coverage(metadata)\n",
    "\n",
    "    # Extract band configuration\n",
    "    bands = extract_band_configuration(metadata)\n",
    "\n",
    "    # Create STAC Item\n",
    "    item = Item(\n",
    "        id=item_id,\n",
    "        geometry=geometry,\n",
    "        bbox=bbox,\n",
    "        datetime=midpoint_datetime,\n",
    "        properties={\n",
    "            \"title\": title,\n",
    "            \"description\": description,\n",
    "            \"license\": license_normalized,\n",
    "            \"start_datetime\": start_datetime.isoformat() + \"Z\",\n",
    "            \"end_datetime\": end_datetime.isoformat() + \"Z\",\n",
    "            \"keywords\": keywords,\n",
    "            \"providers\": providers,\n",
    "            \"msft:region\": \"US\",\n",
    "            \"msft:short_description\": (\n",
    "                \"HLS burn scars imagery and masks for US (2018-2021)\"\n",
    "            ),\n",
    "            \"gsd\": 30,  # Ground sample distance in meters (Landsat/Sentinel-2)\n",
    "            \"platform\": \"Landsat-8, Sentinel-2\",\n",
    "            \"instruments\": [\"OLI\", \"TIRS\", \"MSI\"],\n",
    "            \"constellation\": \"HLS\",\n",
    "            \"dataset_size\": \"804 scenes\",\n",
    "            \"image_size\": \"512x512 pixels\",\n",
    "            \"hls:bands\": (\n",
    "                bands\n",
    "                if bands\n",
    "                else [\n",
    "                    {\n",
    "                        \"name\": \"Blue\",\n",
    "                        \"common_name\": \"blue\",\n",
    "                        \"hls_band\": \"B02\",\n",
    "                        \"wavelength\": \"490nm\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Green\",\n",
    "                        \"common_name\": \"green\",\n",
    "                        \"hls_band\": \"B03\",\n",
    "                        \"wavelength\": \"560nm\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"Red\",\n",
    "                        \"common_name\": \"red\",\n",
    "                        \"hls_band\": \"B04\",\n",
    "                        \"wavelength\": \"665nm\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"NIR\",\n",
    "                        \"common_name\": \"nir\",\n",
    "                        \"hls_band\": \"B8A\",\n",
    "                        \"wavelength\": \"865nm\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"SW1\",\n",
    "                        \"common_name\": \"swir1\",\n",
    "                        \"hls_band\": \"B11\",\n",
    "                        \"wavelength\": \"1610nm\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"SW2\",\n",
    "                        \"common_name\": \"swir2\",\n",
    "                        \"hls_band\": \"B12\",\n",
    "                        \"wavelength\": \"2190nm\",\n",
    "                    },\n",
    "                ]\n",
    "            ),\n",
    "            \"format\": \"TIFF\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Add extensions\n",
    "    item.stac_extensions.extend(\n",
    "        [\n",
    "            \"https://stac-extensions.github.io/table/v1.2.0/schema.json\",\n",
    "            \"https://stac-extensions.github.io/scientific/v1.0.0/schema.json\",\n",
    "            \"https://schemas.stacspec.org/v1.1.0/item-spec/json-schema/item.json\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Add scientific extension\n",
    "    scientific_ext = ScientificExtension.ext(item, add_if_missing=True)\n",
    "    doi = metadata.get(\"doi\", \"\")\n",
    "    if doi and doi.strip():\n",
    "        scientific_ext.doi = doi\n",
    "    scientific_ext.citation = metadata.get(\"citeAs\", \"\")\n",
    "\n",
    "    # Add data collection information\n",
    "    data_collection = metadata.get(\"dataCollection\", {})\n",
    "    if data_collection and dataset_url and doi and doi.strip():\n",
    "        from pystac.extensions.scientific import Publication\n",
    "\n",
    "        # Create citation from data collection info\n",
    "        citation = (\n",
    "            f\"{data_collection.get('name', '')}.\"\n",
    "            f\" {data_collection.get('description', '')} Available at: {dataset_url}\"\n",
    "        )\n",
    "        publication = Publication(doi=doi, citation=citation)\n",
    "        scientific_ext.publications = [publication]\n",
    "\n",
    "    # Add additional metadata from dataCollection\n",
    "    if data_collection:\n",
    "        # Add collection method and sites\n",
    "        if data_collection.get(\"collectionMethod\"):\n",
    "            item.properties[\"collection_method\"] = data_collection[\"collectionMethod\"]\n",
    "\n",
    "        # Add collection sites\n",
    "        collection_sites = data_collection.get(\"collectionSites\", [])\n",
    "        if collection_sites:\n",
    "            item.properties[\"collection_sites\"] = collection_sites\n",
    "\n",
    "    # Add assets from Croissant distribution\n",
    "    for dist in metadata.get(\"distribution\", []):\n",
    "        href = dist.get(\"contentUrl\")\n",
    "        if not href:\n",
    "            continue\n",
    "\n",
    "        asset_id = dist.get(\"@id\", dist.get(\"name\", \"asset\")).replace(\" \", \"_\").lower()\n",
    "        encoding_format = dist.get(\"encodingFormat\")\n",
    "        desc = dist.get(\"description\", asset_id)\n",
    "\n",
    "        # Determine media type and roles\n",
    "        media_type = determine_media_type(href, asset_id, encoding_format)\n",
    "\n",
    "        # Determine roles based on asset type\n",
    "        roles = [\"data\"]\n",
    "        if \"git\" in href.lower():\n",
    "            roles = [\"metadata\"]\n",
    "        elif \"huggingface\" in href.lower():\n",
    "            roles = [\"metadata\", \"documentation\"]\n",
    "        elif \"tiff\" in href.lower() or \"tif\" in href.lower():\n",
    "            roles = [\"data\", \"visual\"]\n",
    "\n",
    "        # Create asset\n",
    "        asset = Asset(href=href, media_type=media_type, title=desc, roles=roles)\n",
    "\n",
    "        # Add additional properties if available\n",
    "        if dist.get(\"fileSize\"):\n",
    "            asset.extra_fields[\"file:size\"] = dist[\"fileSize\"]\n",
    "        if dist.get(\"md5\"):\n",
    "            asset.extra_fields[\"file:checksum\"] = f\"md5:{dist['md5']}\"\n",
    "\n",
    "        item.add_asset(asset_id, asset)\n",
    "\n",
    "    # Add documentation asset if dataset URL exists\n",
    "    if dataset_url:\n",
    "        item.add_asset(\n",
    "            \"documentation\",\n",
    "            Asset(\n",
    "                href=dataset_url,\n",
    "                title=\"Dataset Documentation\",\n",
    "                media_type=MediaType.HTML,\n",
    "                roles=[\"metadata\", \"documentation\"],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Process record sets to add table schema and extract actual data\n",
    "    table_ext = TableExtension.ext(item, add_if_missing=True)\n",
    "    columns = []\n",
    "    sample_data = []\n",
    "\n",
    "    # Extract file listings from geocr:fileListing\n",
    "    file_listing = metadata.get(\"geocr:fileListing\", {})\n",
    "    images_data = file_listing.get(\"images\", {})\n",
    "\n",
    "    for record_set in metadata.get(\"recordSet\", []):\n",
    "        record_id = record_set.get(\"@id\", \"\")\n",
    "        record_name = record_set.get(\"name\", \"\")\n",
    "\n",
    "        # Add columns for the main record set\n",
    "        if \"hls_burn_scars\" in record_id and \"splits\" not in record_id:\n",
    "            columns = [\n",
    "                {\n",
    "                    \"name\": \"image_path\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Path to the image TIFF file\",\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"annotation_path\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Path to the annotation TIFF file\",\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"split\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Dataset split (train/validation)\",\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"scene_id\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"HLS scene identifier\",\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"date\",\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Acquisition date (YYYYMMDD)\",\n",
    "                },\n",
    "            ]\n",
    "\n",
    "            # Extract ALL data records from file listings\n",
    "            for split_name, file_list in images_data.items():\n",
    "                for file_path in file_list:  # Include ALL files, not just samples\n",
    "                    # Extract scene info from filename\n",
    "                    # Format: training/subsetted_512x512_HLS.S30.T10SDH.2020248.v1.4_merged.tif\n",
    "                    filename = file_path.split(\"/\")[-1]\n",
    "                    parts = filename.split(\".\")\n",
    "                    if len(parts) >= 4:\n",
    "                        # Extract the date from the filename (e.g., 2020248 from T10SDH.2020248.v1.4)\n",
    "                        date_part = parts[3]  # This is the date like \"2020248\"\n",
    "                        scene_id = f\"{parts[2]}.{date_part}\"\n",
    "                        date = date_part\n",
    "\n",
    "                        # Create corresponding annotation path\n",
    "                        annotation_path = file_path.replace(\"_merged.tif\", \"_mask.tif\")\n",
    "\n",
    "                        sample_data.append(\n",
    "                            {\n",
    "                                \"image_path\": file_path,\n",
    "                                \"annotation_path\": annotation_path,\n",
    "                                \"split\": split_name,\n",
    "                                \"scene_id\": scene_id,\n",
    "                                \"date\": date,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "    if columns:\n",
    "        table_ext.columns = columns\n",
    "\n",
    "    # Add complete dataset as embedded data in properties\n",
    "    if sample_data:\n",
    "        item.properties[\"dataset_records\"] = sample_data\n",
    "        item.properties[\"total_records\"] = len(sample_data)\n",
    "\n",
    "    # Add complete file listing information as embedded data\n",
    "    if images_data:\n",
    "        file_listings = {}\n",
    "        total_files = 0\n",
    "        for split_name, file_list in images_data.items():\n",
    "            file_listings[split_name] = {\n",
    "                \"count\": len(file_list),\n",
    "                \"files\": file_list,  # Include ALL files, not just examples\n",
    "            }\n",
    "            total_files += len(file_list)\n",
    "\n",
    "        item.properties[\"file_listings\"] = file_listings\n",
    "        item.properties[\"total_files\"] = total_files\n",
    "\n",
    "    # Output or return result\n",
    "    if output_path:\n",
    "        item.save_object(dest_href=output_path)\n",
    "        print(f\"STAC item saved to {output_path}\")\n",
    "    else:\n",
    "        return item.to_dict()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    with open(\"croissant.json\", \"r\") as f:\n",
    "        croissant_data = json.load(f)\n",
    "\n",
    "    stac_item = croissant_to_stac_item(croissant_data, output_path=\"stac_item.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01735163-e665-40b1-9fee-4960df57c39f",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bed8d01c-0c65-4ac0-8155-b235bc8bb845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32mThanks for using STAC version 1.1.0!\u001b[0m\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"version\": \"1.1.0\",\n",
      "        \"path\": \"stac_item.json\",\n",
      "        \"schema\": [\n",
      "            \"https://stac-extensions.github.io/table/v1.2.0/schema.json\",\n",
      "            \"https://stac-extensions.github.io/scientific/v1.0.0/schema.json\",\n",
      "            \"https://schemas.stacspec.org/v1.1.0/item-spec/json-schema/item.json\"\n",
      "        ],\n",
      "        \"valid_stac\": true,\n",
      "        \"asset_type\": \"ITEM\",\n",
      "        \"validation_method\": \"default\"\n",
      "    }\n",
      "]\n",
      "\u001b[32m\n",
      "Validation completed in 757.19ms\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!stac-validator stac_item.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a560de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
