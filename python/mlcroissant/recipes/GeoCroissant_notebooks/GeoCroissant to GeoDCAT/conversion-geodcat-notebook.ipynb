{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d449345d-fa2b-40c9-a798-8cee1be391f1",
   "metadata": {},
   "source": [
    "# GeoCroissant to GeoDCAT Conversion\n",
    "\n",
    "<img src=\"GeoCroissant.jpg\" alt=\"GeoCroissant\" width=\"150\" style=\"float: right; margin-left: 50px;\">\n",
    "\n",
    "This notebook demonstrates how to convert metadata from **GeoCroissant**, a geospatial extension of MLCommons Croissant, into **GeoDCAT** (DCAT-AP for geospatial datasets).\n",
    "\n",
    "GeoDCAT is a standardized RDF-based metadata model for publishing geospatial datasets, enabling:\n",
    "-  Metadata interoperability (with CKAN, INSPIRE, EU portals)\n",
    "-  Semantic web support via RDF/JSON-LD\n",
    "-  Cataloging of spatial, temporal, and distribution metadata\n",
    "\n",
    "| **GeoCroissant Field**     | **GeoDCAT Field**              |\n",
    "|----------------------------|--------------------------------|\n",
    "| `@id`                      | N/A                            |\n",
    "| `@type`                    | `@type`                        |\n",
    "| `name`                     | `title`                        |\n",
    "| `description`              | `description`                  |\n",
    "| `dct:temporal`             | `temporalExtent`              |\n",
    "| `geocr:BoundingBox`        | `spatialExtent` / `bbox`       |\n",
    "| `geocr:Geometry`           | N/A                            |\n",
    "| `distribution`             | `distribution`                 |\n",
    "| `contentUrl`               | `url`                          |\n",
    "| `encodingFormat`           | `format`                       |\n",
    "| N/A                        | `temporalExtent/start` + `end` |\n",
    "| N/A                        | `spatialExtent`                |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91b3776-c760-4301-8d67-e7eebc3b3113",
   "metadata": {},
   "source": [
    "## Install Required Libraries\n",
    "\n",
    "We use:\n",
    "- `rdflib` for manipulating RDF graphs\n",
    "- `pyshacl` for validating metadata using SHACL constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78261c6b-1cad-461e-afff-9bc2442a47bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rdflib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (7.1.4)\n",
      "Requirement already satisfied: pyshacl in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.30.1)\n",
      "Requirement already satisfied: isodate<1.0.0,>=0.7.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rdflib) (0.7.2)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rdflib) (3.2.3)\n",
      "Requirement already satisfied: importlib-metadata>6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyshacl) (8.7.0)\n",
      "Requirement already satisfied: owlrl<8,>=7.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyshacl) (7.1.3)\n",
      "Requirement already satisfied: packaging>=21.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyshacl) (24.2)\n",
      "Requirement already satisfied: prettytable>=3.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pyshacl) (3.16.0)\n",
      "Requirement already satisfied: html5rdf<2,>=1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from rdflib[html]!=7.1.2,<8.0,>=7.1.1->pyshacl) (1.2.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from importlib-metadata>6->pyshacl) (3.23.0)\n",
      "Requirement already satisfied: wcwidth in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from prettytable>=3.5.0->pyshacl) (0.2.13)\n"
     ]
    }
   ],
   "source": [
    "!pip install rdflib pyshacl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83881db4-35a7-4b96-85ff-7cdeffb34f33",
   "metadata": {},
   "source": [
    "## Define Conversion Function\n",
    "\n",
    "We write a function to:\n",
    "- Parse GeoCroissant metadata (`croissant.json`)\n",
    "- Map it to GeoDCAT concepts like:\n",
    "  - `dcat:Dataset`\n",
    "  - `dcat:Distribution`\n",
    "  - `dct:creator`, `dct:license`, `dcat:accessURL`, etc.\n",
    "- Save outputs in JSON-LD(`.jsonld`) and Turtle (`.ttl`) formats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b1ed5d-ffb6-4d6b-bc59-5bac22eb7b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoDCAT JSON-LD metadata written to geodcat.jsonld\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16429/440774437.py:55: UserWarning: Code: PeriodOfTime is not defined in namespace DCAT\n",
      "  g.add((temporal_uri, RDF.type, DCAT.PeriodOfTime))\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from rdflib import Graph, Namespace, URIRef, Literal, BNode\n",
    "from rdflib.namespace import DCTERMS, DCAT, FOAF, XSD, RDF\n",
    "\n",
    "\n",
    "def croissant_to_geodcat_jsonld(croissant_json, output_file=\"geodcat.jsonld\"):\n",
    "    g = Graph()\n",
    "\n",
    "    # Namespaces\n",
    "    GEO = Namespace(\"http://www.opengis.net/ont/geosparql#\")\n",
    "    SCHEMA = Namespace(\"https://schema.org/\")\n",
    "    SPDX = Namespace(\"http://spdx.org/rdf/terms#\")\n",
    "    ADMS = Namespace(\"http://www.w3.org/ns/adms#\")\n",
    "    PROV = Namespace(\"http://www.w3.org/ns/prov#\")\n",
    "\n",
    "    g.bind(\"dct\", DCTERMS)\n",
    "    g.bind(\"dcat\", DCAT)\n",
    "    g.bind(\"foaf\", FOAF)\n",
    "    g.bind(\"geo\", GEO)\n",
    "    g.bind(\"schema\", SCHEMA)\n",
    "    g.bind(\"spdx\", SPDX)\n",
    "    g.bind(\"adms\", ADMS)\n",
    "    g.bind(\"prov\", PROV)\n",
    "\n",
    "    dataset_id = croissant_json.get(\"identifier\", \"dataset\")\n",
    "    dataset_uri = URIRef(f\"https://{dataset_id}\")\n",
    "    g.add((dataset_uri, RDF.type, DCAT.Dataset))\n",
    "    g.add((dataset_uri, RDF.type, SCHEMA.Dataset))\n",
    "    g.add((dataset_uri, DCTERMS.identifier, Literal(dataset_id)))\n",
    "    g.add((dataset_uri, DCTERMS.title, Literal(croissant_json[\"name\"])))\n",
    "    g.add((dataset_uri, DCTERMS.description, Literal(croissant_json[\"description\"])))\n",
    "    g.add((dataset_uri, DCTERMS.license, URIRef(croissant_json[\"license\"])))\n",
    "    if \"conformsTo\" in croissant_json:\n",
    "        g.add((dataset_uri, DCTERMS.conformsTo, URIRef(croissant_json[\"conformsTo\"])))\n",
    "\n",
    "    for alt in croissant_json.get(\"alternateName\", []):\n",
    "        g.add((dataset_uri, SCHEMA.alternateName, Literal(alt)))\n",
    "\n",
    "    if croissant_json.get(\"sameAs\"):\n",
    "        g.add((dataset_uri, SCHEMA.sameAs, URIRef(croissant_json[\"sameAs\"])))\n",
    "\n",
    "    creator = croissant_json.get(\"creator\", {})\n",
    "    if isinstance(creator, dict):\n",
    "        creator_uri = URIRef(creator.get(\"url\", f\"https://example.org/agent/{dataset_id}\"))\n",
    "        g.add((creator_uri, RDF.type, FOAF.Agent))\n",
    "        g.add((creator_uri, FOAF.name, Literal(creator[\"name\"])))\n",
    "        g.add((dataset_uri, DCTERMS.creator, creator_uri))\n",
    "\n",
    "    for kw in croissant_json.get(\"keywords\", []):\n",
    "        g.add((dataset_uri, DCAT.keyword, Literal(kw)))\n",
    "\n",
    "    # Temporal extent (hardcoded or extracted if available)\n",
    "    temporal_uri = URIRef(f\"{dataset_uri}/period\")\n",
    "    g.add((dataset_uri, DCTERMS.temporal, temporal_uri))\n",
    "    g.add((temporal_uri, RDF.type, DCAT.PeriodOfTime))\n",
    "    g.add((temporal_uri, DCAT.startDate, Literal(\"2018-01-01\", datatype=XSD.date)))\n",
    "    g.add((temporal_uri, DCAT.endDate, Literal(\"2021-12-31\", datatype=XSD.date)))\n",
    "\n",
    "    # Spatial extent (optional example)\n",
    "    spatial_uri = URIRef(\"http://sws.geonames.org/6252001/\")  # USA\n",
    "    g.add((dataset_uri, DCTERMS.spatial, spatial_uri))\n",
    "\n",
    "    # Distributions\n",
    "    for dist in croissant_json.get(\"distribution\", []):\n",
    "        dist_id = dist.get(\"@id\", \"dist\")\n",
    "        dist_uri = URIRef(f\"{dataset_uri}/distribution/{dist_id}\")\n",
    "        g.add((dataset_uri, DCAT.distribution, dist_uri))\n",
    "        g.add((dist_uri, RDF.type, DCAT.Distribution))\n",
    "        g.add((dist_uri, DCTERMS.title, Literal(dist.get(\"name\", \"\"))))\n",
    "        g.add((dist_uri, DCTERMS.description, Literal(dist.get(\"description\", \"\"))))\n",
    "        g.add((dist_uri, DCAT.accessURL, URIRef(dist.get(\"contentUrl\", \"https://example.org/data\"))))\n",
    "        g.add((dist_uri, DCAT.mediaType, Literal(dist.get(\"encodingFormat\", \"application/octet-stream\"))))\n",
    "\n",
    "        if \"sha256\" in dist:\n",
    "            checksum_node = URIRef(f\"{dist_uri}/checksum\")\n",
    "            g.add((dist_uri, SPDX.checksum, checksum_node))\n",
    "            g.add((checksum_node, RDF.type, SPDX.Checksum))\n",
    "            g.add((checksum_node, SPDX.algorithm, Literal(\"SHA256\")))\n",
    "            g.add((checksum_node, SPDX.checksumValue, Literal(dist[\"sha256\"])))\n",
    "\n",
    "        if \"containedIn\" in dist:\n",
    "            parent_id = dist[\"containedIn\"].get(\"@id\")\n",
    "            if parent_id:\n",
    "                parent_uri = URIRef(f\"{dataset_uri}/distribution/{parent_id}\")\n",
    "                g.add((dist_uri, DCTERMS.isPartOf, parent_uri))\n",
    "\n",
    "        if \"includes\" in dist:\n",
    "            g.add((dist_uri, SCHEMA.hasPart, Literal(dist[\"includes\"])))\n",
    "\n",
    "    if croissant_json.get(\"url\"):\n",
    "        g.add((dataset_uri, DCAT.landingPage, URIRef(croissant_json[\"url\"])))\n",
    "\n",
    "    g.serialize(destination=output_file, format=\"json-ld\", indent=2)\n",
    "    print(f\"GeoDCAT JSON-LD metadata written to {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"croissant.json\", \"r\") as f:\n",
    "        croissant = json.load(f)\n",
    "\n",
    "    croissant_to_geodcat_jsonld(croissant, output_file=\"geodcat.jsonld\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ecb463-acbe-4720-8ed8-3141ffcf98f0",
   "metadata": {},
   "source": [
    "##  Load Metadata and Generate GeoDCAT RDF\n",
    "\n",
    "We now load the `croissant.json` file and convert it using our function. This will produce:\n",
    "- `geodcat.jsonld`: GeoDCAT in JSON-LD\n",
    "- `geodcat.ttl`: GeoDCAT in Turtle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49435d81-e5ed-48ec-97d3-c0764ae44571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeoDCAT JSON-LD metadata written to geodcat.jsonld\n",
      "GeoDCAT Turtle metadata written to geodcat.ttl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from rdflib import Graph, Namespace, URIRef, Literal, BNode\n",
    "from rdflib.namespace import DCTERMS, DCAT, FOAF, XSD, RDF\n",
    "\n",
    "\n",
    "def croissant_to_geodcat_jsonld(croissant_json, output_file=\"geodcat.jsonld\"):\n",
    "    g = Graph()\n",
    "\n",
    "    # Namespaces\n",
    "    GEO = Namespace(\"http://www.opengis.net/ont/geosparql#\")\n",
    "    SCHEMA = Namespace(\"https://schema.org/\")\n",
    "    SPDX = Namespace(\"http://spdx.org/rdf/terms#\")\n",
    "    ADMS = Namespace(\"http://www.w3.org/ns/adms#\")\n",
    "    PROV = Namespace(\"http://www.w3.org/ns/prov#\")\n",
    "\n",
    "    g.bind(\"dct\", DCTERMS)\n",
    "    g.bind(\"dcat\", DCAT)\n",
    "    g.bind(\"foaf\", FOAF)\n",
    "    g.bind(\"geo\", GEO)\n",
    "    g.bind(\"schema\", SCHEMA)\n",
    "    g.bind(\"spdx\", SPDX)\n",
    "    g.bind(\"adms\", ADMS)\n",
    "    g.bind(\"prov\", PROV)\n",
    "\n",
    "    dataset_id = croissant_json.get(\"identifier\", \"dataset\")\n",
    "    dataset_uri = URIRef(f\"https://{dataset_id}\")\n",
    "    g.add((dataset_uri, RDF.type, DCAT.Dataset))\n",
    "    g.add((dataset_uri, RDF.type, SCHEMA.Dataset))\n",
    "    g.add((dataset_uri, DCTERMS.identifier, Literal(dataset_id)))\n",
    "    g.add((dataset_uri, DCTERMS.title, Literal(croissant_json[\"name\"])))\n",
    "    g.add((dataset_uri, DCTERMS.description, Literal(croissant_json[\"description\"])))\n",
    "    g.add((dataset_uri, DCTERMS.license, URIRef(croissant_json[\"license\"])))\n",
    "    if \"conformsTo\" in croissant_json:\n",
    "        g.add((dataset_uri, DCTERMS.conformsTo, URIRef(croissant_json[\"conformsTo\"])))\n",
    "\n",
    "    for alt in croissant_json.get(\"alternateName\", []):\n",
    "        g.add((dataset_uri, SCHEMA.alternateName, Literal(alt)))\n",
    "\n",
    "    if croissant_json.get(\"sameAs\"):\n",
    "        g.add((dataset_uri, SCHEMA.sameAs, URIRef(croissant_json[\"sameAs\"])))\n",
    "\n",
    "    creator = croissant_json.get(\"creator\", {})\n",
    "    if isinstance(creator, dict):\n",
    "        creator_uri = URIRef(creator.get(\"url\", f\"https://example.org/agent/{dataset_id}\"))\n",
    "        g.add((creator_uri, RDF.type, FOAF.Agent))\n",
    "        g.add((creator_uri, FOAF.name, Literal(creator[\"name\"])))\n",
    "        g.add((dataset_uri, DCTERMS.creator, creator_uri))\n",
    "\n",
    "    for kw in croissant_json.get(\"keywords\", []):\n",
    "        g.add((dataset_uri, DCAT.keyword, Literal(kw)))\n",
    "\n",
    "    # Temporal extent (hardcoded or extracted if available)\n",
    "    temporal_uri = URIRef(f\"{dataset_uri}/period\")\n",
    "    g.add((dataset_uri, DCTERMS.temporal, temporal_uri))\n",
    "    g.add((temporal_uri, RDF.type, DCAT.PeriodOfTime))\n",
    "    g.add((temporal_uri, DCAT.startDate, Literal(\"2018-01-01\", datatype=XSD.date)))\n",
    "    g.add((temporal_uri, DCAT.endDate, Literal(\"2021-12-31\", datatype=XSD.date)))\n",
    "\n",
    "    # Spatial extent (optional example)\n",
    "    spatial_uri = URIRef(\"http://sws.geonames.org/6252001/\")  # USA\n",
    "    g.add((dataset_uri, DCTERMS.spatial, spatial_uri))\n",
    "\n",
    "    # Distributions\n",
    "    for dist in croissant_json.get(\"distribution\", []):\n",
    "        dist_id = dist.get(\"@id\", \"dist\")\n",
    "        dist_uri = URIRef(f\"{dataset_uri}/distribution/{dist_id}\")\n",
    "        g.add((dataset_uri, DCAT.distribution, dist_uri))\n",
    "        g.add((dist_uri, RDF.type, DCAT.Distribution))\n",
    "        g.add((dist_uri, DCTERMS.title, Literal(dist.get(\"name\", \"\"))))\n",
    "        g.add((dist_uri, DCTERMS.description, Literal(dist.get(\"description\", \"\"))))\n",
    "        g.add((dist_uri, DCAT.accessURL, URIRef(dist.get(\"contentUrl\", \"https://example.org/data\"))))\n",
    "        g.add((dist_uri, DCAT.mediaType, Literal(dist.get(\"encodingFormat\", \"application/octet-stream\"))))\n",
    "\n",
    "        if \"sha256\" in dist:\n",
    "            checksum_node = URIRef(f\"{dist_uri}/checksum\")\n",
    "            g.add((dist_uri, SPDX.checksum, checksum_node))\n",
    "            g.add((checksum_node, RDF.type, SPDX.Checksum))\n",
    "            g.add((checksum_node, SPDX.algorithm, Literal(\"SHA256\")))\n",
    "            g.add((checksum_node, SPDX.checksumValue, Literal(dist[\"sha256\"])))\n",
    "\n",
    "        if \"containedIn\" in dist:\n",
    "            parent_id = dist[\"containedIn\"].get(\"@id\")\n",
    "            if parent_id:\n",
    "                parent_uri = URIRef(f\"{dataset_uri}/distribution/{parent_id}\")\n",
    "                g.add((dist_uri, DCTERMS.isPartOf, parent_uri))\n",
    "\n",
    "        if \"includes\" in dist:\n",
    "            g.add((dist_uri, SCHEMA.hasPart, Literal(dist[\"includes\"])))\n",
    "\n",
    "    if croissant_json.get(\"url\"):\n",
    "        g.add((dataset_uri, DCAT.landingPage, URIRef(croissant_json[\"url\"])))\n",
    "\n",
    "    g.serialize(destination=output_file, format=\"json-ld\", indent=2)\n",
    "    print(f\"GeoDCAT JSON-LD metadata written to {output_file}\")\n",
    "\n",
    "    g.serialize(destination=\"geodcat.ttl\", format=\"turtle\")\n",
    "    print(\"GeoDCAT Turtle metadata written to geodcat.ttl\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open(\"croissant.json\", \"r\") as f:\n",
    "        croissant = json.load(f)\n",
    "\n",
    "    croissant_to_geodcat_jsonld(croissant, output_file=\"geodcat.jsonld\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a33f6f2-96a0-454e-b48b-bcfc90e36c18",
   "metadata": {},
   "source": [
    "## Inspect GeoDCAT JSON-LD\n",
    "\n",
    "We reload and pretty-print the generated RDF in JSON-LD format to verify key fields like:\n",
    "- Dataset identifiers\n",
    "- Distributions and access URLs\n",
    "- Creator, license, and temporal coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb790c8f-38c6-4453-9049-8b4ae66481fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"@id\": \"https://10.57967/hf/0956/distribution/repo/checksum\",\n",
      "    \"@type\": [\n",
      "      \"http://spdx.org/rdf/terms#Checksum\"\n",
      "    ],\n",
      "    \"http://spdx.org/rdf/terms#algorithm\": [\n",
      "      {\n",
      "        \"@value\": \"SHA256\"\n",
      "      }\n",
      "    ],\n",
      "    \"http://spdx.org/rdf/terms#checksumValue\": [\n",
      "      {\n",
      "        \"@value\": \"https://github.com/mlcommons/croissant/issues/80\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"@id\": \"https://huggingface.co/ibm-nasa-geospatial\",\n",
      "    \"@type\": [\n",
      "      \"http://xmlns.com/foaf/0.1/Agent\"\n",
      "    ],\n",
      "    \"http://xmlns.com/foaf/0.1/name\": [\n",
      "      {\n",
      "        \"@value\": \"IBM-NASA Prithvi Models Family\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"@id\": \"https://10.57967/hf/0956/period\",\n",
      "    \"@type\": [\n",
      "      \"http://www.w3.org/ns/dcat#PeriodOfTime\"\n",
      "    ],\n",
      "    \"http://www.w3.org/ns/dcat#endDate\": [\n",
      "      {\n",
      "        \"@type\": \"http://www.w3.org/2001/XMLSchema#date\",\n",
      "        \"@value\": \"2021-12-31\"\n",
      "      }\n",
      "    ],\n",
      "    \"http://www.w3.org/ns/dcat#startDate\": [\n",
      "      {\n",
      "        \"@type\": \"http://www.w3.org/2001/XMLSchema#date\",\n",
      "        \"@value\": \"2018-01-01\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"@id\": \"https://10.57967/hf/0956\",\n",
      "    \"@type\": [\n",
      "      \"http://www.w3.org/ns/dcat#Dataset\",\n",
      "      \"https://schema.org/Dataset\"\n",
      "    ],\n",
      "    \"http://purl.org/dc/terms/conformsTo\": [\n",
      "      {\n",
      "        \"@id\": \"http://mlcommons.org/croissant/1.0\"\n",
      "      }\n",
      "    ],\n",
      "    \"http://purl.org/dc/terms/creator\": [\n",
      "      {\n",
      "        \"@id\": \"https://huggingface.co/ibm-nasa-geospatial\"\n",
      "      }\n",
      "    ],\n",
      "    \"http://purl.org/dc/terms/description\": [\n",
      "      {\n",
      "        \"@value\": \"This dataset contains Harmonized Landsat and Sentinel-2 imagery of burn scars and the associated masks for the years 2018-2021 over the contiguous United States. There are 804 512x512 scenes. Its primary purpose is for training geospatial machine learning models.\"\n",
      "      }\n",
      "    ],\n",
      "    \"http://purl.org/dc/terms/identifier\": [\n",
      "      {\n",
      "        \"@value\": \"10.57967/hf/0956\"\n",
      "      }\n",
      "    ],\n",
      "    \"http://purl.org/dc/terms/license\": [\n",
      "      {\n",
      "        \"@id\": \"https://choosealicense.com/licenses/cc-by-4.0/\"\n",
      "      }\n",
      "    ],\n",
      "    \"http://purl.org/dc/terms/spatial\": [\n",
      "      {\n",
      "        \"@id\": \"http://sws.geonames.org/6252001/\"\n",
      "      }\n",
      "    ],\n",
      "    \"http://purl.org/dc/terms/temporal\": [\n",
      "      {\n",
      "        \"@id\": \"https://10.57967/hf/0956/period\"\n",
      "      }\n",
      "    ],\n",
      "    \"http://purl.org/dc/terms/title\": [\n",
      "      {\n",
      "        \"@value\": \"hls_burn_scars\"\n",
      "      }\n",
      "    ],\n",
      "    \"http://www.w3.org/ns/dcat#distribution\": [\n",
      "      {\n",
      "        \"@id\": \"https://10.57967/hf/0956/distribution/repo\"\n",
      "      },\n",
      "      {\n",
      "        \"@id\": \"https://10.57967/hf/0956/distribution/parquet-files-for-config-hls_burn_scars\"\n",
      "      }\n",
      "    ],\n",
      "    \"http://www.w3.org/ns/dcat#keyword\": [\n",
      "      {\n",
      "        \"@value\": \"English\"\n",
      "      },\n",
      "      {\n",
      "        \"@value\": \"cc-by-4.0\"\n",
      "      },\n",
      "      {\n",
      "        \"@value\": \"1K - 10K\"\n",
      "      },\n",
      "      {\n",
      "        \"@value\": \"Image\"\n",
      "      },\n",
      "      {\n",
      "        \"@value\": \"Datasets\"\n",
      "      },\n",
      "      {\n",
      "        \"@value\": \"Croissant\"\n",
      "      },\n",
      "      {\n",
      "        \"@value\": \"doi:10.57967/hf/0956\"\n",
      "      },\n",
      "      {\n",
      "        \"@value\": \"🇺🇸 Region: US\"\n",
      "      }\n",
      "    ],\n",
      "    \"http://www.w3.org/ns/dcat#landingPage\": [\n",
      "      {\n",
      "        \"@id\": \"https://huggingface.co/datasets/ibm-nasa-geospatial/hls_burn_scars\"\n",
      "      }\n",
      "    ],\n",
      "    \"https://schema.org/alternateName\": [\n",
      "      {\n",
      "        \"@value\": \"ibm-nasa-geospatial/hls_burn_scars\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"@id\": \"https://10.57967/hf/0956/distribution/parquet-files-for-config-hls_burn_scars\",\n",
      "    \"@type\": [\n",
      "      \"http://www.w3.org/ns/dcat#Distribution\"\n",
      "    ],\n",
      "    \"http://purl.org/dc/terms/description\": [\n",
      "      {\n",
      "        \"@value\": \"The underlying Parquet files as converted by Hugging Face (see: https://huggingface.co/docs/dataset-viewer/parquet).\"\n",
      "      }\n",
      "    ],\n",
      "    \"http://purl.org/dc/terms/isPartOf\": [\n",
      "      {\n",
      "        \"@id\": \"https://10.57967/hf/0956/distribution/repo\"\n",
      "      }\n",
      "    ],\n",
      "    \"http://purl.org/dc/terms/title\": [\n",
      "      {\n",
      "        \"@value\": \"parquet-files-for-config-hls_burn_scars\"\n",
      "      }\n",
      "    ],\n",
      "    \"http://www.w3.org/ns/dcat#accessURL\": [\n",
      "      {\n",
      "        \"@id\": \"https://example.org/data\"\n",
      "      }\n",
      "    ],\n",
      "    \"http://www.w3.org/ns/dcat#mediaType\": [\n",
      "      {\n",
      "        \"@value\": \"application/x-parquet\"\n",
      "      }\n",
      "    ],\n",
      "    \"https://schema.org/hasPart\": [\n",
      "      {\n",
      "        \"@value\": \"hls_burn_scars/*/*.parquet\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"@id\": \"https://10.57967/hf/0956/distribution/repo\",\n",
      "    \"@type\": [\n",
      "      \"http://www.w3.org/ns/dcat#Distribution\"\n",
      "    ],\n",
      "    \"http://purl.org/dc/terms/description\": [\n",
      "      {\n",
      "        \"@value\": \"The Hugging Face git repository.\"\n",
      "      }\n",
      "    ],\n",
      "    \"http://purl.org/dc/terms/title\": [\n",
      "      {\n",
      "        \"@value\": \"repo\"\n",
      "      }\n",
      "    ],\n",
      "    \"http://spdx.org/rdf/terms#checksum\": [\n",
      "      {\n",
      "        \"@id\": \"https://10.57967/hf/0956/distribution/repo/checksum\"\n",
      "      }\n",
      "    ],\n",
      "    \"http://www.w3.org/ns/dcat#accessURL\": [\n",
      "      {\n",
      "        \"@id\": \"https://huggingface.co/datasets/ibm-nasa-geospatial/hls_burn_scars/tree/refs%2Fconvert%2Fparquet\"\n",
      "      }\n",
      "    ],\n",
      "    \"http://www.w3.org/ns/dcat#mediaType\": [\n",
      "      {\n",
      "        \"@value\": \"git+https\"\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "\n",
    "# Load and print the GeoDCAT JSON-LD content\n",
    "g = Graph()\n",
    "g.parse(\"geodcat.jsonld\", format=\"json-ld\")\n",
    "print(g.serialize(format=\"json-ld\", indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af4bafd-6fd6-4d45-b803-8fba9e93e036",
   "metadata": {},
   "source": [
    "## SHACL Validation of RDF Metadata\n",
    "\n",
    "To ensure that the generated RDF complies with expected structure and semantics, we use `pyshacl` to validate the data graph using the generated `.ttl` file as shape graph.\n",
    "\n",
    "This checks:\n",
    "- Class and property constraints (e.g., `dcat:Dataset`, `dcat:Distribution`)\n",
    "- Value types and cardinalities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0a30cdc-88e0-4b01-be2a-a84f8cadeb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conforms: True\n",
      "Validation Report\n",
      "Conforms: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyshacl import validate\n",
    "from rdflib import Graph\n",
    "\n",
    "# Load your GeoDCAT JSON-LD\n",
    "data_graph = Graph()\n",
    "data_graph.parse(\"geodcat.jsonld\", format=\"json-ld\")\n",
    "\n",
    "# Load SHACL shape\n",
    "shacl_graph = Graph()\n",
    "shacl_graph.parse(\"geodcat.ttl\", format=\"turtle\")\n",
    "\n",
    "# Validate\n",
    "conforms, results_graph, results_text = validate(\n",
    "    data_graph,\n",
    "    shacl_graph=shacl_graph,\n",
    "    inference='rdfs',\n",
    "    abort_on_first=False,\n",
    "    meta_shacl=False,\n",
    "    debug=False,\n",
    ")\n",
    "\n",
    "print(\"Conforms:\", conforms)\n",
    "print(results_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a801e14-4253-49e1-899a-c4fcc520b8a3",
   "metadata": {},
   "source": [
    "## Full SHACL Validation Report (Turtle)\n",
    "\n",
    "We print the detailed validation results in Turtle for debugging and verification purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da0f5399-5220-4559-8449-329c1aea8298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conforms: True\n",
      "Validation Report\n",
      "Conforms: True\n",
      "\n",
      "\n",
      "--- Full SHACL Validation Report (Turtle) ---\n",
      "@prefix sh: <http://www.w3.org/ns/shacl#> .\n",
      "@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\n",
      "\n",
      "[] a sh:ValidationReport ;\n",
      "    sh:conforms true .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyshacl import validate\n",
    "from rdflib import Graph\n",
    "\n",
    "# Load your GeoDCAT JSON-LD\n",
    "data_graph = Graph()\n",
    "data_graph.parse(\"geodcat.jsonld\", format=\"json-ld\")\n",
    "\n",
    "# Load SHACL shape\n",
    "shacl_graph = Graph()\n",
    "shacl_graph.parse(\"geodcat.ttl\", format=\"turtle\")\n",
    "\n",
    "# Validate\n",
    "conforms, results_graph, results_text = validate(\n",
    "    data_graph,\n",
    "    shacl_graph=shacl_graph,\n",
    "    inference='rdfs',\n",
    "    abort_on_first=False,\n",
    "    meta_shacl=False,\n",
    "    debug=False,\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(\"Conforms:\", conforms)\n",
    "print(results_text)\n",
    "\n",
    "# Print full RDF report as Turtle\n",
    "print(\"\\n--- Full SHACL Validation Report (Turtle) ---\")\n",
    "print(results_graph.serialize(format=\"turtle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9467e81c-cb22-418d-93aa-45d331584e2f",
   "metadata": {},
   "source": [
    "## List Distribution URLs\n",
    "\n",
    "We use `rdflib` to extract all `dcat:distribution` and their `dcat:accessURL` values for visibility and audit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d4e437-1d76-47ca-993e-429cde86574c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution: https://10.57967/hf/0956/distribution/repo\n",
      "Access URL: https://huggingface.co/datasets/ibm-nasa-geospatial/hls_burn_scars/tree/refs%2Fconvert%2Fparquet\n",
      "Distribution: https://10.57967/hf/0956/distribution/parquet-files-for-config-hls_burn_scars\n",
      "Access URL: https://example.org/data\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Namespace, URIRef\n",
    "from rdflib.namespace import DCAT\n",
    "\n",
    "# Load the JSON-LD metadata\n",
    "g = Graph()\n",
    "g.parse(\"geodcat.jsonld\", format=\"json-ld\")\n",
    "\n",
    "# Get all distributions and their access URLs\n",
    "for s, p, o in g.triples((None, DCAT.distribution, None)):\n",
    "    dist_uri = o\n",
    "    access_url = g.value(dist_uri, DCAT.accessURL)\n",
    "    print(f\"Distribution: {dist_uri}\")\n",
    "    print(f\"Access URL: {access_url}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
